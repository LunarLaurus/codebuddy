# config.yaml

# URL of your local LLM server
local_llm_url: "http://localhost:8000"

# Default maximum tokens for summarization requests
default_max_tokens: 768

default_timeout_second: 60.0

# Default temperature for LLM generation
default_temperature: 0.3
